Templates theme : Ion boardroom

Má»¥c tiÃªu cá»§a má»—i pháº§n trong slide :

I, Thu tháº­p vÃ  xá»­ lÃ½ dá»¯ liá»‡u : TrÃ¬nh bÃ y nguá»“n dá»¯ liá»‡u, phÃ¢n tÃ­ch chá»n káº¿t quáº£,  thuá»™c tÃ­nh xá»­ lÃ½ tiá»n xá»­ lÃ½ dá»¯ liá»‡u  , tá»•ng káº¿t

[?] Gradient

Gradient lÃ  vector chá»‰ hÆ°á»›ng Ä‘áº¡o hÃ m lÃ  má»™t sá»‘ vÃ´ hÆ°á»›ng, nÃ³ thá»ƒ hiá»‡n ráº±ng hÆ°á»›ng dáº«n tá»›i cá»±c trá»‹ (cá»±c tiá»ƒu hoáº·c cá»±c Ä‘áº¡i)
trong máº¡ng neural lÃ  giÃ¡ trá»‹ cá»±c tiá»ƒu (Loss function)



[?] Gradient descent 
LÃ  thuáº­t toÃ¡n tÃ¬m kiáº¿m sá»± thay Ä‘á»•i trá»ng sá»‘ sao cho bÆ°á»›c tiáº¿p theo sáº½ giáº£m Ä‘Æ°á»£c lá»—i, hoáº·c lÃ  giáº£m Ä‘i gradient á»Ÿ láº§n sau 
CÃ´ng thá»©c chung:
	xnew = xold - learningrate.gradient(x)

Æ¯u Ä‘iá»ƒm :
â€‚ğŸ‘†Thuáº­t toÃ¡n gradient descent cÆ¡ báº£n, dá»… hiá»ƒu. Thuáº­t toÃ¡n Ä‘Ã£ giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á» tá»‘i Æ°u model neural network báº±ng cÃ¡ch cáº­p nháº­t trá»ng sá»‘ sau má»—i vÃ²ng láº·p.
NhÆ°á»£c Ä‘iá»ƒm :
â€‚ğŸ‘†VÃ¬ Ä‘Æ¡n giáº£n nÃªn thuáº­t toÃ¡n Gradient Descent cÃ²n nhiá»u háº¡n cháº¿ nhÆ° phá»¥ thuá»™c vÃ o nghiá»‡m khá»Ÿi táº¡o ban Ä‘áº§u vÃ  learning rate.
â€‚ğŸ‘†VÃ­ dá»¥ 1 hÃ m sá»‘ cÃ³ 2 global minimum thÃ¬ tÃ¹y thuá»™c vÃ o 2 Ä‘iá»ƒm khá»Ÿi táº¡o ban Ä‘áº§u sáº½ cho ra 2 nghiá»‡m cuá»‘i cÃ¹ng khÃ¡c nhau.
â€‚ğŸ‘†Tá»‘c Ä‘á»™ há»c quÃ¡ lá»›n sáº½ khiáº¿n cho thuáº­t toÃ¡n khÃ´ng há»™i tá»¥, quanh quáº©n bÃªn Ä‘Ã­ch vÃ¬ bÆ°á»›c nháº£y quÃ¡ lá»›n; hoáº·c tá»‘c Ä‘á»™ há»c nhá» áº£nh hÆ°á»Ÿng Ä‘áº¿n tá»‘c Ä‘á»™ training


[?] Momentum
Háº¡n cháº¿ cá»§a gradient lÃ  khÃ´ng tÃ¬m Ä‘Æ°á»£c Ä‘iá»ƒm min toÃ n cá»¥c Ä‘á»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y ngÆ°á»i ta sá»­ dá»¥ng thÃªm quÃ¡n tÃ­nh 
CÃ´ng thá»©c : 
	xnew = xold -(gama.v + learningrate.gradient)
Æ¯u Ä‘iá»ƒm :
â€‚ğŸ‘†Thuáº­t toÃ¡n tá»‘i Æ°u giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á»: Gradient Descent khÃ´ng tiáº¿n Ä‘Æ°á»£c tá»›i Ä‘iá»ƒm global minimum mÃ  chá»‰ dá»«ng láº¡i á»Ÿ local minimum.
NhÆ°á»£c Ä‘iá»ƒm :
â€‚ğŸ‘†Tuy momentum giÃºp hÃ²n bi vÆ°á»£t dá»‘c tiáº¿n tá»›i Ä‘iá»ƒm Ä‘Ã­ch, tuy nhiÃªn khi tá»›i gáº§n Ä‘Ã­ch, nÃ³ váº«n máº¥t khÃ¡ nhiá»u thá»i gian giao Ä‘á»™ng qua láº¡i trÆ°á»›c khi dá»«ng háº³n, Ä‘iá»u nÃ y Ä‘Æ°á»£c giáº£i thÃ­ch vÃ¬ viÃªn bi cÃ³ Ä‘Ã .

[?] Adagrad
hÃ´ng giá»‘ng nhÆ° cÃ¡c thuáº­t toÃ¡n trÆ°á»›c Ä‘Ã³ thÃ¬ learning rate háº§u nhÆ° giá»‘ng nhau trong quÃ¡ trÃ¬nh training (learning rate lÃ  háº±ng sá»‘), Adagrad coi learning rate lÃ  1 tham sá»‘. 
Tá»©c lÃ  Adagrad sáº½ cho learning rate biáº¿n thiÃªn sau má»—i thá»i Ä‘iá»ƒm t
n  : háº±ng sá»‘
gt : gradient táº¡i thá»i Ä‘iá»ƒm t
Ïµ  : há»‡ sá»‘ trÃ¡nh lá»—i ( chia cho máº«u báº±ng 0)
G  : lÃ  ma tráº­n chÃ©o mÃ  má»—i pháº§n tá»­ trÃªn Ä‘Æ°á»ng chÃ©o (i,i) lÃ  bÃ¬nh phÆ°Æ¡ng cá»§a Ä‘áº¡o hÃ m vectÆ¡ tham sá»‘ táº¡i thá»i Ä‘iá»ƒm t.
Æ¯u Ä‘iá»ƒm :
â€‚ğŸ‘†Má»™t lÆ¡i Ã­ch dá»… tháº¥y cá»§a Adagrad lÃ  trÃ¡nh viá»‡c Ä‘iá»u chá»‰nh learning rate báº±ng tay, chá»‰ cáº§n Ä‘á»ƒ tá»‘c Ä‘á»™ há»c default lÃ  0.01 thÃ¬ thuáº­t toÃ¡n sáº½ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh.
NhÆ°á»£c Ä‘iá»ƒm :
â€‚ğŸ‘†Yáº¿u Ä‘iá»ƒm cá»§a Adagrad lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng biáº¿n thiÃªn sáº½ lá»›n dáº§n theo thá»i gian cho Ä‘áº¿n khi nÃ³ lÃ m tá»‘c Ä‘á»™ há»c cá»±c kÃ¬ nhá», lÃ m viá»‡c training trá»Ÿ nÃªn Ä‘Ã³ng bÄƒng.

[?] Adam 
KhÃ´ng máº¥t thá»i gian dao Ä‘á»™ng táº¡i Ä‘iá»ƒm cá»±c trá»‹ ná»¯a vÃ¬ nÃ³ cÃ³ sá»©c náº·ng


[?]

Regression is method dealing with linear dependencies, neural networks can deal with nonlinearities. So if your data will have some nonlinear dependencies, 
neural networks should perform better than regression.
Translate : Há»“i quy tuyáº¿n tÃ­nh vá»›i cÃ¡c váº¥n Ä‘á» cÃ³ tÃ­nh tuyáº¿n tÃ­nh, vá»›i máº¡ng neural cÃ³ thá»ƒ dá»± Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» phi tuyáº¿n , vÃ¬ váº­y náº¿u dá»¯ liá»‡u cÃ³ tÃ­nh cháº¥t phi tuyáº¿n thÃ¬ máº¡ng neural 
sáº½ cÃ³ hiá»‡u xuáº¥t tá»‘t hÆ¡n há»“i quy tuyáº¿n tÃ­nh




** Thuáº­t toÃ¡n gradient descent:

The gradient descent algorithm seeks to change the weights so that the next evaluation reduces the error, 
meaning the optimization algorithm is navigating down the gradient (or slope) of error.


** Lá»±a chá»n thuá»™c tÃ­nh cho model:

Giáº£m thá»i gian tran ing 
Loáº¡i bá» trÆ°á»ng khÃ´ng liÃªn quan Ä‘áº¿n dá»¯ liá»‡u
TÄƒng Ä‘á»™ chÃ­nh xÃ¡c cho mÃ´ hÃ¬nh

PhÆ°Æ¡ng phÃ¡p :
	Filter method   : Má»‘i tÆ°Æ¡ng quan thuá»™c tÃ­nh vÃ  giÃ¡ trá»‹ Ä‘áº§u ra
	Wrapper method  : So sÃ¡nh sá»± há»¯u Ã­ch cá»§a táº­p con thuá»™c tÃ­nh
	Intrinsic method

